% to-do
% -----
% - write
% - distinguish error from uncertainty and frequency from probability

% style notes
% -----------
% - careful with the words ``error'' and ``uncertainty''
% - careful with the words ``probability'' and ``frequency''
% - use () for function arguments, and [] for grouping/precedence
% - define macros; remember 1, 2, infinity

\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs}

\newcommand{\notenglish}[1]{\textit{#1}}
\newcommand{\aposteriori}{\notenglish{a~posteriori}}
\newcommand{\apriori}{\notenglish{a~priori}}

\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\problemname}{Exercise}
\newcommand{\commentsname}{Comments}

\newcounter{problem}
\newenvironment{problem}{\paragraph{\problemname~\theproblem:}\refstepcounter{problem}}{}
\newenvironment{comments}{\paragraph{\commentsname:}}{}

% matrix stuff
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
\newcommand{\mA}{\mmatrix{A}}
\newcommand{\mAT}{\transpose{\mA}}
\newcommand{\mC}{\mmatrix{C}}
\newcommand{\mCinv}{\inverse{\mC}}
\newcommand{\mX}{\mmatrix{X}}
\newcommand{\mY}{\mmatrix{Y}}
\newcommand{\mYT}{\transpose{\mY}}

% set stuff
\newcommand{\setofall}[3]{\{{#1}\}_{{#2}}^{{#3}}}
\newcommand{\allq}{\setofall{q_i}{i=1}{N}}
\newcommand{\ally}{\setofall{y_i}{i=1}{N}}

% other random multiply used math symbols
\renewcommand{\d}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\pgood}{p_{\mathrm{good}}}

\begin{document}
\section*{Data analysis recipes:\ \\
  Fitting a straight line to data\footnote{
    Copyright 2009 David~W.~Hogg (david.hogg@nyu.edu).  You may copy
    and distribute this document provided that you make no changes to
    it whatsoever.}}

\noindent
David~W.~Hogg\\
\textsl{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,\\
New~York~University}\\
\texttt{david.hogg@nyu.edu}

\begin{abstract}
  I go through all of the considerations involved in fitting a
  straight line to a set of points in a two-dimensional plane.
  Standard chi-squared fitting is only appropriate when there is a
  dimension along which the data points have negligible uncertainties;
  this condition is rarely met in practice.  In addition to
  considering cases of general, heterogeneous, and arbitrarily
  covariant two-dimensional uncertainties, I also look at situations
  in which there are bad data (large outliers), upper or lower limits
  on some points, unknown uncertainties, and unknown but expected
  intrinsic scatter in the linear relationship being fit.  Above all I
  emphasize the importance of choosing a justified scalar objective,
  and recommend separating that decision from any decisions about the
  details of optimization.
\end{abstract}

Common problem.

No consensus on generalizations.

Absurdities in the literature.

Robot science: I want methods that are so reliable that they can be
performed automatically by unsupervised robots on millions of
problems.

Apologies for the astronomy-centrism.

\section{Standard practice}\label{sec:standard}

You have a set of $N>2$ points $(x_i,y_i)$, with known gaussian
uncertainties $\sigma_{yi}$ in the $y$ direction, and no uncertainty
at all (that is, perfect knowledge) in the $x$ direction.  You want to
find the function $f(x)$ of the form
\begin{equation}\label{eq:fofx}
f(x) = m\,x + b \quad ,
\end{equation}
where $m$ is the slope and $b$ is the intercept, that ``best fits''
the points.  What is meant by ``best fits'' is, of course, very
important, and in what follows I will have a lot to say about that.
For now, I describe standard practice:

Construct the matrices
\begin{equation}
\mY = \left[\begin{array}{c}
y_1 \\
y_2 \\
\cdots \\
y_N
\end{array}\right] \quad ,
\end{equation}
\begin{equation}
\mA = \left[\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\multicolumn{2}{c}{\cdots} \\
1 & x_N
\end{array}\right] \quad ,
\end{equation}
\begin{equation}
\mC = \left[\begin{array}{cccc}
\sigma_{y1}^{2} & 0 & \cdots & 0 \\
0 & \sigma_{y2}^{2} & \cdots & 0 \\
\multicolumn{4}{c}{\cdots} \\
0 & 0 & \cdots & \sigma_{yN}^{2}
\end{array}\right] \quad ,
\end{equation}
where one might call $\mY$ a ``vector'', and the covariance matrix
$\mC$ can be generalized to the case in which there are covariances
between the different $y$-direction uncertainties.  The best-fit
values for the parameters $m$ and $b$ are just the components of a
column vector $\mX$ found by
\begin{equation}\label{eq:lsf}
\left[\begin{array}{c} $b$ \\ $m$ \end{array}\right]
 = \mX = \inverse{\left[\mAT\,\mCinv\,\mA\right]}
  \,\left[\mAT\,\mCinv\,\mY\right] \quad .
\end{equation}
This seems all very complicated, but it is actually the simplest thing
that can be written down that is linear, obeys matrix multiplication
rules, and has the right relative sensitivity to data of different
statistical significance.  It can be justified in one of several ways;
the linear algebra justification starts by noting that you want to
solve the equation
\begin{equation}
\mY = \mA\,\mX \quad ,
\end{equation}
but you can't because that equation is over-constrained.  So you
weight everything with the inverse of the covariance matrix (as you
would if you were doing, say, a weighted average), and then
left-multiply everything by $\mAT$ to reduce the dimensionality, and
then \equationname~(\ref{eq:lsf}) is the solution of that
reduced-dimensionality equation.

This methodology minimizes a quantity $\chi^2$ (``chi-squared''),
which is the total squared error, scaled by the uncertainties, or
\begin{equation}\label{eq:chisquared}
\chi^2
 = \sum_{i=1}^N \frac{\left[y_i - f(x_i)\right]^2}{\sigma_{yi}^2}
 \equiv \transpose{\left[\mY-\mA\,\mX\right]}
 \,\mCinv\,\left[\mY-\mA\,\mX\right]
 \quad ,
\end{equation}
that is, it finds the values for $m$ and $b$ that minimize $\chi^2$.
This, of course, is only one possible meaning of the phrase ``best
fit''.

When the uncertainties are gaussian and their variances $\sigma_{yi}$
are correctly estimated, the matrix
$\inverse{\left[\mAT\,\mCinv\,\mA\right]}$ that appears in
\equationname~(\ref{eq:lsf}) is just the covariance matrix (gaussian
uncertainty variances on the diagonal, covariances off the diagonal)
for the parameters in $\mX$.  The justification of this will have to
wait for a discussion of the objective function, if it comes at all.

\begin{comments}
Even when the conditions of standard practice are met, it is
\emph{still} often done wrong!  It is not unusual to see the
individual data-point error estimates ignored, even when they are
known at least approximately.  It is also common for the problem to
get ``transposed'' such that the coordinates for which errors are
negligible (the independent variables) are put into the $\mY$ vector
and the coordinates for which errors are \emph{not} negligible (the
dependent variables) are put into the $\mA$ matrix.  In this latter
case, the procedure makes no sense at all really; it happens when the
investigator thinks of some quantity ``really being'' the dependent
variable, despite the fact that it has the smaller error.  In the
context of fitting, however, there is no meaning to these
``independent'' and ``dependent'' terms beyond the error properties.
In performing this standard fit, the investigator is effectively
assuming that the $x$ values have negligible uncertainties; if they do
\emph{not}, then the investigator is making a mistake.

In the above, cite the papers on the Hubble expansion from SNe.

Physicists should be studying linear algebra and computation at least
as much as they study calculus and differential equations.  Discuss.

The inverse covariance matrix is like a linear ``metric'' for the data
space.

Note that not only does this problem have a linear solution, the
solution is convex or unique.  This is remarkable, and almost no
perturbation of this problem (as we will see below) has either of
these properties, let alone both.
\end{comments}

\begin{problem}\label{prob:standard}
Using the standard linear algebra method of this \sectionname, fit the
straight line $y=m\,x+b$ to the $x$, $y$, and $\sigma_y$ values in
\tablename~??.  Make a plot showing the points, their uncertainties,
and the best-fit line.  What is the standard uncertainty variance
$\sigma_m^2$ on the slope of the line?  Is there anything you don't
like about the result?
\end{problem}

\begin{problem}\label{prob:quadratic}
Generalize the method of this \sectionname\ to fit a general quadratic
(second order) relationship.  Add another column to matrix $\mA$
containing the values $x_i^2$, and another element to vector $\mX$
(call it $q$).  Then re-do \problemname~\ref{prob:standard} but
fitting for and plotting the best quadratic relationship
\begin{equation}
g(x) = q\,x^2 + m\,x + b \quad.
\end{equation}
\end{problem}

\begin{problem}
Explain in words why or how the linear algebra expression in
\equationname~(\ref{eq:lsf}) minmizes the squared error.  If it helps,
consider only the case in which all the data points have identical
uncertainties (so the matrix $\mC$ is trivial).
\end{problem}

\section{The objective function}

A scientist's justification of \equationname~(\ref{eq:lsf}) cannot
appeal purely to abstract ideas of linear algebra, but must originate
from the scientific question at hand.  Here and in what follows, I
will advocate that the only reliable procedure is to use all one's
knowledge about the problem to construct a (preferably) justified,
(necessarily) scalar (or, really, one-dimensional), \emph{objective
  function} that represents monotonically the quality of the fit.  In
this framework, fitting anything to anything involves a scientific
question about the objective function representing ``goodness of fit''
and then a separate and subsequent engineering question about how to
\emph{find the optimum} and, possibly, the confidence interval or
posterior probability distribution around that optimum.  Note that in
the above section we did \emph{not} proceed according to these rules;
indeed the procedure was introduced prior to the objective function,
and the objective function was not justified.

One method for finding or creating a justified scalar objective is to
make a ``generative model'' for the data, or a statistical model for
how a data set similar to the one you have might have been generated.
In the case of the straight line fit in the presence of known,
gaussian uncertainties, one can create this generative model by
imagining that the data \emph{really do} come from a line of the form
$y = f(x) = m\,x+b$, and that the only reason that any data point
deviates from this perfect, straight line is that to this has been
added a small $y$-direction offset drawn from a gaussian distribution
of zero mean and known variance $\sigma_y^2$.  In this model, given an
independent position $x_i$, an uncertainty $\sigma_{yi}$, a slope $m$,
and an intercept $b$, the frequency distribution
$p(y_i|x_i,\sigma_{yi},m,b)$ for $y_i$ is
\begin{equation}
p(y_i|x_i,\sigma_{yi},m,b) = \frac{1}{\sqrt{2\,\pi\,\sigma_{yi}^2}}
 \,\exp\left(-\frac{[y_i - m\,x - b]^2}{2\,\sigma_{yi}^2}\right) \quad ,
\end{equation}
where this gives the expected frequency (in a hypothetical set of
repeated experiments) of getting value in the range $[y_i,y_i+\d y]$
per unit $\d y$.

This generative model provides us with a natural, justified, scalar
objective: We seek the line (parameters $m$ and $b$) that maximize the
likelihood of the observed data.  The likelihood $\like$ is
\begin{equation}\label{eq:like}
\like \propto \prod_{i=1}^N p(y_i|x_i,\sigma_{yi},m,b) \quad ,
\end{equation}
where I have used ``$\propto$'' not ``$=$'' to be careful about the
infinitesimal volume $(\d y)^N$ and a possible normalization constant
that comes from the overall probability of the data in the context of
the model space (to which I may return below).  Taking the logarithm,
\begin{eqnarray}\displaystyle
\ln\like
 & = & K + \sum_{i=1}^N -\frac{[y_i - m\,x - b]^2}{2\,\sigma_{yi}^2} \nonumber\\
 & = & K - \frac{1}{2}\,\chi^2 \quad ,
\end{eqnarray}
where $K$ is some constant.  This shows that likelihood maximization
is identical to $\chi^2$ minimization and we have justified,
scientifically, the procedure of the previous section.

The Bayesian generalization of this is to say that
\begin{equation}
p(m,b|\ally,I) = \frac{p(\ally|m,b,I)}{p(\ally|I)}\,p(m,b|I) \quad ,
\end{equation}
where $m$ and $b$ are the model parameters, $\ally$ is a short-hand for
all the data $y_i$, $I$ is a short-hand for all the prior knowledge of
the $x_i$ and the $\sigma_{yi}$ and everything else about the problem,
$p(m,b|\ally,I)$ is the \emph{posterior} probability distribution for the
parameters given the data and the prior knowledge, the ratio is the
likelihood just computed, which has a frequency distribution for the
data on top and that same thing marginalized over all parameters on
the bottom (the denominator can be thought of as a normalization
constant), and $p(m,b|I)$ is the \emph{prior} probability distribution
for the parameters that represents all knowledge \emph{except} the
input data $\ally$.  Unless the prior $p(m,b|I)$ is pretty informative,
the posterior distribution function $p(m,b|\ally,I)$ here is going to look
very similar to the likelihood function in
\equationname~(\ref{eq:like}) above.

We have succeeded in justifying the standard method as optimizing a
justified, scalar objective function.  It is just the great good luck
of Gaussian distributions that that optimization is a pure linear
function of the data, and therefore trivial to implement.

\begin{comments}
Note the relationship to the weighted mean, the simple result of
combining measurements with gaussian uncertainties?  Why does the
objective have to be a ``scalar''?  Why did we put the $x_i$ and
$\sigma_{yi}$ into $I$ and not into the set of data, and when is that
appropriate?  Why did I talk about the ``frequency distribution'' for
the errors and not the ``probability distribution''?
\end{comments}

\begin{problem}
Imagine a set of $N$ measurements $z_i$, with uncertainty variances
$\sigma_{zi}^2$, all of the same (unknown) quantity $Z$.  Assuming the
generative model that each $z_i$ differs from $Z$ by a
gaussian-distributed offset, taken from a gaussian with zero mean and
variance $\sigma_{zi}^2$, write down an experession for the log
likelihood $\ln\like$ for the data given the model parameter $Z$.
Take a derivative and show that the maximum likelihood value for $Z$
is the usual weighted mean.
\end{problem}

\section{Robustness to outliers}

The standard linear fitting method is very sensitive to
\emph{outliers}, points that are substantially farther from the linear
relation than expected because of unmodeled experimental error or
unmodeled but rare sources of noise.  There are two general approaches
to this problem, which are not necessarily different.  The first is to
``soften'' the objective function at large deviation.  The second is
to find ways to objectively remove or reject ``bad'' points.  Both of
these are strongly preferable to sorting through the data by hand, for
reasons of subjectivity and irreproducibility that I need not state.

One straightforward way to soften the objective function relative to
$\chi^2$ is to lower the power to which residuals are raised.  For
example, if we model the frequency distribution
$p(y_i|x_i,\sigma_{yi},m,b)$ not with a gaussian but rather with a
biexponential
\begin{equation}
p(y_i|x_i,\sigma_{yi},m,b) = \frac{1}{2\,s_i}
 \,\exp\left(-\frac{|y_i-m\,x_i-b|}{s_i}\right) \quad ,
\end{equation}
where $s_i$ is an estimate of the mean absolute error, probably
correctly set to something like $s_i = \sigma_{yi}/\sqrt{2}$.
Optimization of the total log likelihood is equivalent to minimizing
\begin{equation}\label{eq:biexp}
X = \sum_{i=1}^N \frac{|y_i-f(x_i)|}{s_i} \quad ,
\end{equation}
where $f(x)$ is the straight line of \equationname~\ref{eq:fofx}.
This approach is rarely quantitatively justified, but it has the nice
property that it introduces no new parameters.

Another straightforward softening is to (smoothly) cut off the
contribution of a residual as it becomes large.  For example, replace
$\chi^2$ with
\begin{equation}\label{eq:soft}
\chi_Q^2 = \sum_{i=1}^N \frac{Q^2\,[y_i-f(x_i)]^2}
  {Q^2\,\sigma_{yi}^2+[y_i-f(x_i)]^2} \quad ,
\end{equation}
where $Q^2$ is the maximum amount a point can contribute to $\chi_Q^2$
(cite papers cited by Barron).  When each residual is small, its
contribution to $\chi_Q^2$ is nearly identical to its contribution to
the standard $\chi^2$, but as the residual gets substantial, the
contribution of the residual does not increase as the square.  This is
about the simplest robust method that introduces only one new
parameter ($Q$).

The alternative to these softenings is to try data rejection, in which
adds to the problem a set of $N$ binary integers $q_i$, one per data
point, each of which is unity if the $i$th data point is good, and
zero if the $i$th data point is bad (cite Press and Jaynes here).  In
addition, to construct an objective function one needs an additional
parameter $\pgood$, which is the \emph{prior} probability that any
individual data point is good.  All these $N+1$ extra parameters may
seem like crazy baggage, but their values can be \emph{inferred} and
\emph{marginalized out} so in the end, this method introduces \emph{no
  new parameters}.

In this situation, I don't know a consistent way to proceed that isn't
Bayesian (possibly for lack of trying), so let's go Bayesian.  In this
case, likelihood is
\begin{eqnarray}\displaystyle
\frac{p(\ally|m,b,\allq,\pgood,I)}{p(\ally|I)}
 &\propto& \prod_{i=1}^N \left[\exp\left(-\frac{[y_i-m\,x_i-b]^2}{2\,\sigma_{yi}^2}\right)\right]^q_i \quad ,
\end{eqnarray}
but there is an important prior probability
\begin{eqnarray}\displaystyle
p(m,b,\allq,\pgood|I)
 &=& p(m,b|I)\,p(\allq|\pgood,I)\,p(\pgood|I) \nonumber \\
 &=& p(m,b|I)\,\pgood^{q_i}\,[1-\pgood]^{[1-q_i]} \quad ,
\end{eqnarray}
where I have assumed that the prior probability distribution
$p(\pgood|I)$ for $\pgood$ is just flat on the interval $0<\pgood<1$
(the least informative possible assumption).

The posterior probability distribution is the product of the
(correctly normalized) likelihood times the prior.  If all we care
about are the parameters $(m,b)$ of the line, we have to marginalize
the posterior probability distribution over all choices for the binary
integers $\allq$ and all possible values for $\pgood$.  This
marginalization involves evaluating and marginalizing (over $\pgood$)
$2^N$ different likelihoods!  Of course because very few points are
true candidates for rejection, there are many ways to ``trim the
tree'' and do this quickly; the lazy can simply sample; the very lazy
can loop over all $2^N$ and go on vacation while it executes.

One unfortunate (though unavoidably correct) thing about any Bayesian
method is that it does not return an ``answer'' but rather it returns
a \emph{posterior probability distribution}.  Strictly, this posterior
distribution function \emph{is} your answer.  However, what scientists
are usually doing is not, strictly, inference, but rather,
decision-making.  That is, the investigator wants an answer, not a
distribution.  There are (at least) two reactions to this.  One is to
ignore the fundamental Bayesianism at the end, and choose simply the
``maximum \aposteriori'' (MAP) answer---the single value of $(m,b)$
that optimizes the fully marginalized (over $\pgood$ and $\allq$)
posterior probability distribution function.  This is the Bayesian's
analog of maximum likelihood.  The other reaction is to suck it up and
sample the posterior probability distribution and carry forward not
one answer to the problem but $M$ answers, each of which is drawn
fairly and independently from the posterior distribution function.
The latter is to be preferred because \textsl{(a)}~it shows the
uncertainties very clearly, and \textsl{(b)}~the sampling can be
carried forward to future inferences as an approximation to the
posterior distribution function, useful for propagating uncertainty,
or standing in as a \emph{prior} for some subsequent inference.

\begin{comments}
No robust procedure will be linear, so everything must be iterated to
convergence.  Almost nothing robust will be convex, even, so there are
issues about local minima.

On bad data, you might not want to give all points the same prior
probability distribution function over $\pgood$.  One of the amusing
things about the posterior is that you can pick a particular data
point $I$ and marginalize over $(m,b)$ and all the $q_i$ \emph{except}
$q_I$.  This will return the marginalized posterior probability that
point $I$ is good.  This is good for embarassing colleagues in
meta-analyses (cite Press here).

The standard method for removing sensitivity to outliers (in
astrophysics, anyway) is known as ``sigma-clipping''.  This is a
procedure that involves performing the fit, identifying the worst
outliers in a $\chi^2$ sense---the points that contribute more than
some threshold $Q^2$ to the $\chi^2$ sum, removing those, fitting
again, identifying again, and so on to convergence.  This procedure is
easy to implement, fast, and reliable (provided that the threshold
$Q^2$ is set high enough), but it has various problems that make it
less suitable than the methods described above.  One is that it is a
\emph{procedure} not an \emph{objective function}.  The procedure does
not necessarily optimize a justifiable objective.  A second is that
the procedure gives an answer that depends, in general, on the
starting point or initialization, and because there is there is no way
to compare different answers (there is no objective function), the
investigator can't decide which of two converged answers is
``better''.  You might think that the answer with least scatter
(smallest $\chi^2$ per data point) is better, but that will favor
solutions that involve rejecting most of the data; you might think the
answer that uses the most data is better, but that can have a very
large $\chi^2$.  These issues relate to the fact that the method does
not explicitly \emph{penalize} the rejection of data; this is another
bad consequence of not having an explicit objective function.  A third
problem is that the procedure does not necessarily converge to
anything non-trivial at all; if the threshold $Q^2$ gets very small,
there are situations in which all but two of the data points can be
rejected by the procedure.  All that said, with a large $Q^2$ and
standard, pretty good data, the sigma-clipping procedure is easy to
implement and fast; I have often used it myself.
\end{comments}

\begin{problem}
Using the biexponential objective function of
\equationname~(\ref{eq:biexp}), find the best-fit straight line
$y=m\,x+b$ for the $x$, $y$, and $\sigma_y$ values in \tablename~??.
You will have to use a non-linear optimizer of some kind or you will
have to take a sampling approach.  Make a plot showing the points,
their uncertainties, and the best-fit line.  How does this compare to
the standard result you obtained in \problemname~\ref{prob:standard}?
Do you like it better or worse?
\end{problem}

\begin{problem}
Using the soft chi-squared objective function of
\equationname~(\ref{eq:soft}), find the best-fit straight line
$y=m\,x+b$ for the $x$, $y$, and $\sigma_y$ values in \tablename~??.
You will have to use a non-linear optimizer of some kind or you will
have to take a sampling approach.  Alternatively, you can try an
iterated-linear method in which you iterate the standard linear method
but modify the weights at each iteration.  Make a plot showing the
points, their uncertainties, and the best-fit line.  How does this
compare to the standard result you obtained in
\problemname~\ref{prob:standard}?  Do you like it better or worse?
\end{problem}

\begin{problem}
Using the fully marginalized Bayesian data rejection method described
above, find the best-fit (the maximum \aposteriori) straight line
$y=m\,x+b$ for the $x$, $y$, and $\sigma_y$ values in \tablename~??.
Before choosing the MAP line, marginalize over $\pgood$ and $\allq$ by
(dumbly) looping over every possible (non-trivial) assigment of $q_i$
values, and at each assignment, integrate over all possible values of
$\pgood$.  Make a plot showing the points, their uncertainties, and
the MAP line.  How does this compare to the standard result you
obtained in \problemname~\ref{prob:standard}?  Do you like the MAP
line better or worse?  For extra credit, plot a sampling of 10 lines
drawn from the marginalized posterior distribution for $(m,b)$
(marginalized over $\pgood$ and $\allq$) and plot the samples as a set
of light grey or transparent lines.  For extra extra credit, mark each
data point on your plot with the fully marginalized probability that
the point is good (that is, not rejected, or has $q=1$).
\end{problem}

\section{Assigning uncertainties to best-fit parameters}

In the standard linear-algebra method of $\chi^2$ minimization given
in \sectionname~\ref{sec:standard}, the uncertainties in the best-fit
parameters $(m,b)$ are given by the two-dimensional output covariance
matrix
\begin{equation}
\left[\begin{array}{cc}
\sigma_{b}^2 & \sigma_{mb}^2 \\
\sigma_{mb}^2 & \sigma_{m}^2
\end{array}\right] = \inverse{\left[\mAT\,\mCinv\,\mA\right]} \quad ,
\end{equation}
where the ordering is defined by the ordering in matrix $\mA$.  But
these best-fit uncertainties \emph{only} strictly hold under three
extremely strict conditions, almost none of which is met in real
practice: \textsl{(a)}~The uncertainties in the data points must truly
be gaussian, with variances correctly described by the
$\sigma_{yi}^2$, \textsl{(b)}~there must be no rejection of any data
or any departure from the exact, standard definition of $\chi^2$ given
in \equationname~(\ref{eq:chisquared}), and \textsl{(c)}~the
generative model of the data implied by the method---that is, that the
data are truly drawn from a negligible-scatter linear relationship and
subsequently had noise added, where the noise offsets were generated
by a gaussian process---must be an accurate description of the data.

These conditions are rarely met in practice.  Often the noise
estimates are rough, the uncertainties are known not to be gaussian,
the investigator has applied data rejection or equivalent
conditioning, and the relationship has intrinsic scatter and
curvature.  For these generic reasons, we much prefer empirical
estimates of the uncertainty in the best-fit parameters.

%%% syntactical reference point

\section{Non-gaussian errors}

Errors and uncertainties.

What you are really assuming.

Generalized error distributons.

\section{Arbitrary two-dimensional uncertainties}

\begin{comments}
Bash the forward and reverse fitting in Tully-Fisher.
\end{comments}

\section{Limits}

\section{Intrinsic scatter}

\section{Discussion}

\paragraph{Acknowlegments}
It is a pleasure to thank Sam Roweis (Toronto), who taught me to find
and optimize objective functions.  This research was partially
supported by NASA (ADP grant NNX08AJ48G) and a Research Fellowship of
the Alexander von Humboldt Foundation.

\end{document}
