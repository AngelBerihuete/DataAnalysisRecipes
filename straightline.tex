% to-do
% - write
% - use () for functions, and [] for grouping/precedence
% - distinguish error from uncertainty and frequency from probability

\documentclass[12pt]{article}
\usepackage{amssymb,amsmath,mathrsfs}

\renewcommand{\d}{\mathrm{d}}
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
\newcommand{\mA}{\mmatrix{A}}
\newcommand{\mAT}{\transpose{\mA}}
\newcommand{\mC}{\mmatrix{C}}
\newcommand{\mCinv}{\inverse{\mC}}
\newcommand{\mX}{\mmatrix{X}}
\newcommand{\mY}{\mmatrix{Y}}
\newcommand{\mYT}{\transpose{\mY}}
\newcommand{\like}{\mathscr{L}}

\begin{document}
\section*{Data analysis recipes:\ \\
  Fitting a straight line to data\footnote{
    Copyright 2009 David~W.~Hogg (david.hogg@nyu.edu).  You may copy
    and distribute this document provided that you make no changes to
    it whatsoever.}}

\noindent
David~W.~Hogg\\
\textsl{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics,\\
New~York~University}\\
\texttt{david.hogg@nyu.edu}

\begin{abstract}
  In excruciating detail, I go through all of the considerations
  involved in fitting a straight line to a set of points in a
  two-dimensional plane.  Standard chi-squared fitting is only
  appropriate when there is a dimension along which the data points
  have negligible uncertainties; this condition is rarely met in
  practice.  In addition to considering cases of general,
  heterogeneous, and arbitrarily covariant two-dimensional
  uncertainties, I also look at situations in which there are large
  outliers, upper or lower limits on some points, unknown
  uncertainties, and unknown but expected intrinsic scatter in the
  linear relationship being fit.  Above all I emphasize the importance
  of choosing a justified scalar objective, and recommend separating
  that decision from any decisions about the details of optimization.
\end{abstract}

Common problem.

No consensus on generalizations.

Absurdities in the literature.

Robot science: I want methods that are so reliable that they can be
performed automatically by unsupervised robots on millions of
problems.

\section{Standard practice}

You have a set of $N>2$ points $(x_i,y_i)$, with known gaussian
uncertainties $\sigma_{yi}$ in the $y$ direction, and no uncertainty
at all (that is, perfect knowledge) in the $x$ direction.  You want to
find the function $f(x)$ of the form
\begin{equation}
f(x) = m\,x + b \quad ,
\end{equation}
where $m$ is the slope and $b$ is the intercept, that ``best fits''
the points.  What is meant by ``best fits'' is, of course, very
important, and in what follows I will have a lot to say about that.
For now, I describe standard practice:

Construct the matrices
\begin{equation}
\mY = \left[\begin{array}{c}
y_1 \\
y_2 \\
\cdots \\
y_N
\end{array}\right] \quad ,
\end{equation}
\begin{equation}
\mA = \left[\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\multicolumn{2}{c}{\cdots} \\
1 & x_N
\end{array}\right] \quad ,
\end{equation}
\begin{equation}
\mC = \left[\begin{array}{cccc}
\sigma_{y1}^{2} & 0 & \cdots & 0 \\
0 & \sigma_{y2}^{2} & \cdots & 0 \\
\multicolumn{4}{c}{\cdots} \\
0 & 0 & \cdots & \sigma_{yN}^{2}
\end{array}\right] \quad ,
\end{equation}
where one might call $\mY$ a ``vector'', and the covariance matrix
$\mC$ can be generalized to the case in which there are covariances
between the different $y$-direction uncertainties.  The best-fit
values for the parameters $m$ and $b$ are just the components of a
column vector $\mX$ found by
\begin{equation}\label{eq:lsf}
\left[\begin{array}{c} $b$ \\ $m$ \end{array}\right]
 = \mX = \inverse{\left(\mAT\,\mCinv\,\mA\right)}
  \,\left(\mAT\,\mCinv\,\mY\right) \quad .
\end{equation}
This seems all very complicated, but it is actually the simplest thing
that can be written down that is linear, obeys matrix multiplication
rules, and has the right relative sensitivity to data of different
statistical significance.  It can be justified in one of several ways;
the linear algebra justification starts by noting that you want to
solve the equation
\begin{equation}
\mY = \mA\,\mX \quad ,
\end{equation}
but you can't because that equation is over-constrained.  So you
weight everything with the inverse of the covariance matrix (as you
would if you were doing, say, a weighted average), and then
left-multiply everything by $\mAT$ to reduce the dimensionality, and
then equation~(\ref{eq:lsf}) is the solution of that
reduced-dimensionality equation.

This methodology minimizes a quantity $\chi^2$ (``chi-squared''),
which is the mean square error, scaled by the uncertainties, or
\begin{equation}
\chi^2
 = \sum_{i=1}^N \frac{\left(y_i - f(x_i)\right)^2}{\sigma_{yi}^2}
 \equiv \transpose{\left(\mY-\mA\,\mX\right)}\,\mCinv\,\left(\mY-\mA\,\mX\right)
 \quad ,
\end{equation}
that is, it finds the values for $m$ and $b$ that minimize $\chi^2$.
This, of course, is only one possible meaning of the phrase ``best
fit''.

When the uncertainties are gaussian and their variances $\sigma_{yi}$
are correctly estimated, the matrix
$\inverse{\left(\mAT\,\mCinv\,\mA\right)}$ that appears in
equation~\ref{eq:lsf} is just the covariance matrix (gaussian
uncertainty variances on the diagonal, covariances off the diagonal)
for the parameters in $\mX$.  The justification of this will have to
wait for a discussion of the objective function, if it comes at all.

Note that even when the conditions of standard practice are met, it is
\emph{still} often done wrong!  It is not unusual to see the
individual data-point error estimates ignored, even when they are
known at least approximately.  It is also common for the problem to
get ``transposed'' such that the coordinates for which errors are
negligible (the independent variables) are put into the $\mY$ vector
and the coordinates for which errors are \emph{not} negligible (the
dependent variables) are put into the $\mA$ matrix.  In this latter
case, the procedure makes no sense at all really; it happens when the
investigator thinks of some quantity ``really being'' the dependent
variable, despite the fact that it has the smaller error.  In the
context of fitting, however, there is no meaning to these
``independent'' and ``dependent'' terms beyond the error properties.
In performing this standard fit, the investigator is effectively
assuming that the $x$ values have negligible uncertainties; if they do
\emph{not}, then the investigator is making a mistake.

\section{The objective function}

A scientist's justification of equation~(\ref{eq:lsf}) cannot appeal
purely to abstract ideas of linear algebra, but must originate from
the scientific question at hand.  Here and in what follows, I will
advocate that the only reliable procedure is to use all one's
knowledge about the problem to construct a (preferably) justified,
(necessarily) scalar (or, really, one-dimensional), \emph{objective
function} that represents monotonically the quality of the fit.  In
this framework, fitting anything to anything involves a scientific
question about the objective function representing ``goodness of fit''
and then a separate and subsequent engineering question about how to
\emph{find the optimum} and, possibly, the confidence interval or
posterior probability distribution around that optimum.  Note that in
the above section we did \emph{not} proceed according to these rules;
indeed the procedure was introduced prior to the objective function,
and the objective function was not justified.

One method for finding or creating a justified scalar objective is to
make a ``generative model'' for the data, or a statistical model for
how a data set similar to the one you have might have been generated.
In the case of the straight line fit in the presence of known,
gaussian uncertainties, one can create this generative model by
imagining that the data \emph{really do} come from a line of the form
$y = f(x) = m\,x+b$, and that the only reason that any data point
deviates from this perfect, straight line is that to this has been
added a small $y$-direction offset drawn from a gaussian distribution
of zero mean and known variance $\sigma_y^2$.  In this model, given an
independent position $x_i$, an uncertainty $\sigma_{yi}$, a slope $m$,
and an intercept $b$, the frequency distribution
$p(y_i|x_i,\sigma_{yi},m,b)$ for $y_i$ is
\begin{equation}
p(y_i|x_i,\sigma_{yi},m,b) = \frac{1}{\sqrt{2\,\pi\,\sigma_{yi}^2}}
 \,\exp\left(-\frac{(y_i - m\,x - b)^2}{2\,\sigma_{yi}^2}\right) \quad ,
\end{equation}
where this gives the expected frequency (in a hypothetical set of
repeated experiments) of getting value in the range $[y_i,y_i+\d y]$
per unit $\d y$.

This generative model provides us with a natural, justified, scalar objective:  We seek the line (parameters $m$ and $b$) that maximize the likelihood of the observed data.  The likelihood $\like$ is 
\begin{equation}\label{eq:like}
\like \propto \prod_{i=1}^N p(y_i|x_i,\sigma_{yi},m,b) \quad ,
\end{equation}
where I have used ``$\propto$'' not ``$=$'' to be careful about the
infinitesimal volume $(\d y)^N$ and a possible normalization constant
that comes from the overall probability of the data in the context of
the model space (to which I may return below).  Taking the logarithm,
\begin{eqnarray}\displaystyle
\ln\like
 & = & K + \sum_{i=1}^N -\frac{(y_i - m\,x - b)^2}{2\,\sigma_{yi}^2} \nonumber\\
 & = & K - \frac{1}{2}\,\chi^2 \quad ,
\end{eqnarray}
where $K$ is some constant.  This shows that likelihood maximization
is identical to $\chi^2$ minimization and we have justified,
scientifically, the procedure of the previous section.

The Bayesian generalization of this is to say that
\begin{equation}
p(m,b|D,I) = \frac{p(D|m,b,I)}{p(D|I)}\,p(m,b|I) \quad ,
\end{equation}
where $m$ and $b$ are the model parameters, $D$ is a short-hand for
all the data $y_i$, $I$ is a short-hand for all the prior knowledge of
the $x_i$ and the $\sigma_{yi}$ and everything else about the problem,
$p(m,b|D,I)$ is the \emph{posterior} probability distribution for the
parameters given the data and the prior knowledge, the ratio is the
likelihood just computed, which has a frequency distribution for the
data on top and that same thing marginalized over all parameters on
the bottom (the denominator can be thought of as a normalization
constant), and $p(m,b|I)$ is the \emph{prior} probability distribution
for the parameters that represents all knowledge \emph{except} the
input data $D$.  Unless the prior $p(m,b|I)$ is pretty informative,
the posterior distribution function $p(m,b|D,I)$ here is going to look
very similar to the likelihood function in equation~(\ref{eq:like})
above.

We have succeeded in justifying the standard method as optimizing a
justified, scalar objective function.  It is just the great good luck
of Gaussian distributions that that optimization is a pure linear
function of the data, and therefore trivial to implement.

\section{A comment on non-gaussian errors}

Errors and uncertainties.

What you are really assuming.

Fitting for the error distribution.  Etc.

\section{Arbitrary two-dimensional uncertainties}

\section{Robustness to outliers}

Criticize sigma-clipping relative to having a known scalar objective.
Sigma-clipping is a procedure the end point of which depends on the
starting point.

\section{Limits}

\section{Unknown datapoint uncertainties}

\section{Intrinsic scatter}

\section{Discussion}

\paragraph{Acknowlegments}
It is a pleasure to thank Sam Roweis (Toronto), who taught me to find
and optimize objective functions.  This research was partially
supported by NASA (ADP grant NNX08AJ48G) and a Research Fellowship of
the Alexander von Humboldt Foundation.

\end{document}
