\documentclass[12pt,twoside]{article}
\usepackage{amssymb,amsmath,mathrsfs,deluxetable}
\usepackage{natbib}
\usepackage{float,graphicx}
% hypertex insanity
\usepackage{color,hyperref}
\definecolor{linkcolor}{rgb}{0,0,0.25}
\hypersetup{
  colorlinks=true,        % false: boxed links; true: colored links
  linkcolor=linkcolor,    % color of internal links
  citecolor=linkcolor,    % color of links to bibliography
  filecolor=linkcolor,    % color of file links
  urlcolor=linkcolor      % color of external links
}
%%Figure caption
\makeatletter
\newsavebox{\tempbox}
\newcommand{\@makefigcaption}[2]{%
\vspace{10pt}{#1.--- #2\par}}%
\renewcommand{\figure}{\let\@makecaption\@makefigcaption\@float{figure}}
\makeatother

\setlength{\emergencystretch}{2em}%No overflow

\newcommand{\notenglish}[1]{\textsl{#1}}
\newcommand{\aposteriori}{\notenglish{a~posteriori}}
\newcommand{\apriori}{\notenglish{a~priori}}
\newcommand{\adhoc}{\notenglish{ad~hoc}}
\newcommand{\etal}{\notenglish{et al.}}
\newcommand{\eg}{\notenglish{e.g.}}

\newcommand{\documentname}{document}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\problemname}{Exercise}
\newcommand{\solutionname}{Solution}
\newcommand{\commentsname}{Comments}

\newcounter{problem}
\newenvironment{problem}{\paragraph{\problemname~\theproblem:}\refstepcounter{problem}}{}
\newenvironment{comments}{\paragraph{\commentsname:}}{}

% matrix stuff
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
\newcommand{\mA}{\mmatrix{A}}
\newcommand{\mAT}{\transpose{\mA}}
\newcommand{\mC}{\mmatrix{C}}
\newcommand{\mCinv}{\inverse{\mC}}
\newcommand{\mE}{\mmatrix{E}}
\newcommand{\mQ}{\mmatrix{Q}}
\newcommand{\mS}{\mmatrix{S}}
\newcommand{\mX}{\mmatrix{X}}
\newcommand{\mY}{\mmatrix{Y}}
\newcommand{\mYT}{\transpose{\mY}}
\newcommand{\mZ}{\mmatrix{Z}}
\newcommand{\vhat}{\mmatrix{\hat{v}}}

% parameter vectors
\newcommand{\parametervector}[1]{\mmatrix{\vec{#1}}}
\newcommand{\pvtheta}{\parametervector{\theta}}

% set stuff
\newcommand{\setofall}[3]{\{{#1}\}_{{#2}}^{{#3}}}
\newcommand{\allq}{\setofall{q_i}{i=1}{N}}
\newcommand{\allx}{\setofall{x_i}{i=1}{N}}
\newcommand{\ally}{\setofall{y_i}{i=1}{N}}
\newcommand{\allxy}{\setofall{x_i,y_i}{i=1}{N}}
\newcommand{\allsigmay}{\setofall{\sigma_{yi}^2}{i=1}{N}}
\newcommand{\allC}{\setofall{\mC_i}{i=1}{N}}

% other random multiply used math symbols
\renewcommand{\d}{\mathrm{d}}
\newcommand{\like}{\mathscr{L}}
\newcommand{\Pbad}{P_{\mathrm{b}}}
\newcommand{\Ybad}{Y_{\mathrm{b}}}
\newcommand{\Vbad}{V_{\mathrm{b}}}
\newcommand{\bperp}{b_{\perp}}
\newcommand{\mean}[1]{\left<{#1}\right>}
\newcommand{\meanZ}{\mean{\mZ}}
\newcommand{\best}{\mathrm{best}}

% header stuff
\renewcommand{\MakeUppercase}[1]{#1}
\pagestyle{myheadings}
\renewcommand{\sectionmark}[1]{\markright{\thesection.~#1}}
\markboth{Fitting a straight line to data}{}

\begin{document}
\thispagestyle{plain}\raggedbottom
%% \section*{Data analysis recipes:\ \\
%%   Comparing models of different complexity\footnote{
%%     Copyright 2010 by the authors.
%%     \textbf{This is a DRAFT version dated 2010-03-28.
%%     Please do not distribute this document.}}}
%% %    You may copy and distribute this document
%% %    provided that you make no changes to it whatsoever.}}

%% \noindent
%% David~W.~Hogg\footnote{\textsl{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University} and \textsl{Max-Planck-Institut f\"ur Astronomie, Heidelberg}},
%% Jo~Bovy\footnote{\textsl{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}}, \&
%% Dustin~Lang\footnote{\textsl{Department of Computer Science, University of Toronto} and \textsl{Princeton University Observatory}}

%% \begin{abstract}
%%   TBD
%% \end{abstract}

Imagine a set of $N$ data points $y_i$ which deviate from a
$K$-dimensional (or $K$-parameter) linear model by the addition of
noise samples $e_i$ drawn from Gaussian distributions of (presumed
known) variances $\sigma_{yi}^2$.  The model can be written as
\begin{equation}
y_i = \sum_{j=1}^K
  a_{ij}\,x_j + e_i
  \quad ,
\end{equation}
or, in matrix notation,
\begin{equation}
\mY = \mA\,\mX + \mE
  \quad ,
\end{equation}
where $\mY$ is an $N$-dimensional vector of the $y_i$, $\mA$ is an
$N\times K$ matrix of (presumed known) coefficients $a_{ij}$, $\mX$ is
a $K$-dimensional vector of the parameters $x_j$, and $\mE$ is an
$N$-dimensional vector of the noise draws or errors $e_i$.  The
standard thing to do is to minimize a $\chi^2$ scalar
\begin{equation}
\chi^2 = \sum_{i=1}^N
  \frac{\left[y_i - a_{ij}\,x_j\right]^2}{\sigma_{yi}^2}
  \quad ,
\end{equation}
where this is scientifically justified when all the aforementioned
assumptions (linear model space includes the truth, noise is Gaussian
with known variances, and so on) are true.  This objective ($\chi^2$)
is truly a scalar in the sense that it has a compact matrix
representation
\begin{equation}
\chi^2 = \transpose{\left[\mY-\mA\,\mX\right]}
  \,\mCinv\,\left[\mY-\mA\,\mX\right]
  \quad .
\end{equation}
In this situation, when $N<K$, the best-fit value for the parameters
(the elements of $\mX$) is given by
\begin{equation}
\mX_\best = \inverse{\left[\mAT\,\mCinv\,\mA\right]}
  \,\left[\mAT\,\mCinv\,\mY\right]
  \quad ,
\end{equation}
which was found by forcing the full derivative of the $\chi^2$ scalar
to vanish.  In this formulation, if the (errors of the) $y_i$ are
independent, the covariance matrix $\mC$ is the diagonal $N\times N$
matrix with the noise variances $\sigma_{yi}^2$ on the diagonal.

Frequently (for engineers, if not astronomers) the condition $N<K$
does not hold, but there are smoothness expectations that provide
enough support to set the parameters nonetheless.  The simplest
implementation of a smoothness prior or regularization is the addition
to $\chi^2$ of a quadratic penalty for derivatives in the parameters
$x_j$.  In this picture, we imagine the parameters $x_j$ are ordered
such that nearby $j$ are nearby in the appropriate sense.  The scalar
objective gets modified in this case to
\begin{equation}
\chi_r^2 = \sum_{i=1}^N
  \frac{\left[y_i - a_{ij}\,x_j\right]^2}{\sigma_{yi}^2}
  + \epsilon\,\sum_{j=1}^{K-1}\left[x_{j+1}-x_j\right]^2
  \quad ,
\end{equation}
where $\epsilon$ is a control parameter that controls the smoothness
of the model, or the stiffness of it, or else the model complexity in
some sense.  Under this objective, the best fit parameter vector is
given by
\begin{equation}
\mX_\best = \inverse{\left[\mAT\,\mCinv\,\mA + \epsilon\,\mQ\right]}
  \,\left[\mAT\,\mCinv\,\mY\right]
  \quad ,
\end{equation}
where the $K\times K$ matrix $\mQ$ is a tri-diagonal matrix that looks
like this
\begin{equation}
\mQ = \left[\begin{array}{cccccc}
    1 &-1 & 0 & 0 & 0 & 0 \\
   -1 & 2 &-1 & 0 & 0 & 0 \\
    0 &-1 & 2 &-1 & 0 & 0 \\
    0 & 0 &-1 & 2 &-1 & 0 \\
    0 & 0 & 0 &-1 & 2 &-1 \\
    0 & 0 & 0 & 0 &-1 & 1 \\
  \end{array}\right]
  \quad ,
\end{equation}
in the case of $K=6$ parameters and generalizes in the obvious way for
different $K$.

\end{document}
