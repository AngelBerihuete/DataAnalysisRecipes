% This document is part of the Data Analysis Recipes project.
% Copyright 2020 the author.

% to-do
% -----
% - make up a toy data set and make problems.
%   - make two different generative models for the toy data.
%   - should have bayes and frequentist options.
% - get BibTeX working like GPR.
% - Where does the point go that there are many qualitatively different
%   sources of noise or uncertainty.
% - See notes from Stars & Exoplanets meeting 2020-06-03.
% - Make common style file for both this document and GaussianProductRefactor

\documentclass[10pt]{article}
\usepackage{amsmath, bm, mathrsfs, amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{graphicx}
\usepackage{marginfix} % necessary but possibly evil

% citation stuhh
\usepackage{doi}
\usepackage{natbib}
\bibliographystyle{hogg_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text macros
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}

% math macros
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}

% variables
\newcommand{\va}{\vector{a}}
\newcommand{\vy}{\vector{y}}
\newcommand{\tC}{\tensor{C}}
\newcommand{\mX}{\matrix{X}}

% page layout stuhh
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\parindent}{\baselineskip}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2\textwidth}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\refstepcounter{figure}\textbf{\figurename~\thefigure}: {#1}}}

% and make the left margin correct
\setlength{\oddsidemargin}{0.5\paperwidth}
\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\oddsidemargin}{-0.5\marginparwidth}
\addtolength{\oddsidemargin}{-0.5\marginparsep}

% and the top margin
\setlength{\topmargin}{0.5\paperheight}
\addtolength{\topmargin}{-1.0in}
\addtolength{\topmargin}{-0.5\textheight}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{Data Analysis Recipes:\\
  What is the uncertainty\\ on my measurement?}

\textbf{David W. Hogg}\footnote{%
    It is a pleasure to thank
    the participants in my data-analysis classes in New York City,
    and
    the weekly Stars and Exoplanets meeting at the Flatiron Insititute
    for help with all of this material.} \\
{\footnotesize Center for Cosmology and Particle Physics, Dept Physics, New York University} \\
{\footnotesize Max-Planck-Institut f\"ur Astronomie} \\
{\footnotesize Flatiron Institute}

\paragraph{Abstract:}
Any measurement you make using data ought to be reported with an uncertainty
estimate (often called an ``error'' or an ``error bar'' unfortunately).
I discuss and compare methodologies for making such estimates.
The options availble to you depend on whether you have a generative model for your
data, with which either you can simulate your noisy data, or (even better)
you can compute probability densities for different data sets.
They also depend on whether you believe that model,
or believe what it implies for the moments of the noise distribution.
If you have a good generative model, information theory or data simulations can deliver
measurement uncertainties; if you don't they can often provide strong bounds.
Either way, bootstrap and jackknife methods provide well justified, empirical
alternatives that I recommend.
I also discuss the role of nuisance parameters in uncertainty estimation, and
the differences between Bayesian and frequentist approaches and interpretation.
I spend a bit of time on common issues and troubleshooting.
One important idea is that the circumstances in which an uncertainty can be estimated
precisely are rare: Even when you have a very precise measurement, you probably won't
know the uncertainty on that measurement with great precision.

\section{Measurements and uncertainties}

You have data. There's something you want to measure. You have many
options for making this measurement: You can come up with an
\textsl{estimator}, that transforms your data (through arithmetic
operations) into an estimate of the quantity you want to measure. You
can write down a likelihood function---a probability density function
(or pdf) for your data given the quantity you want to measure (and
maybe other nuisance parameters)---and you can optimize it. That
procedure will get you an estimator, but it would be a
\textsl{maximum-likelihood estimator}, which has some great
properties (to be discussed below).
Or you can write down, in addition to your likelihood
function, a set of prior pdfs over parameters and perform
\textsl{Bayesian inference}.  In each of these cases you will make
some kind of measurement, and in each of these cases you will be
expected to deliver that measurement with an associated \textsl{uncertainty}.

This estimate of your uncertainty can come from different kinds of operations, with
different epistemological status. In some kinds of uncertainty
estimates, we use physical knowledge of the data-generating process to
compute, from (essentially) theory, how the noise in the data
contaminates the measurement. In other kinds of uncertainty estimates,
we use the variance or noise visible in the data to estimate the measurement
uncertainty. Loyal readers of \documentname s in this series can imagine that
I am generally partial to empirical or data-driven uncertainty estimates over
theoretical uncertainty estimates! But I discuss both in detail in what follows,
because (in the physical sciences at least) there are often cases in which the
theoretical uncertainties are very close to correct, or at least very useful.

And---closely related to the above point---the \emph{source} of the
uncertainty can be a well-defined noise process (like photon shot
noise in a detector); or it can be an ill-understood noise process
(like the variabilities of stars, for which we have no good model); or
it can be something else (unknown calibration systematics, mistakes or
wrong approximations in the physical model, and so on). For some analysses
all of these noise processes are relevant; for others only some are.
That is, the way uncertainties are estimated has something to do with
\emph{how they will be used}. This is related to the (sometimes obscure)
ways that physicists separate uncertainties into ``statistical'' and ``systematic''.
And what's ``systematic'' to some users will be ``statistical'' to others.
We will return to that point below.

One of my grad-school mentors\footnote{Gerry Neugebauer (1932--2014),
  who was one of the pioneers of infrared astrophysics, and the US
  lead of the NASA \textsl{IRAS} Mission, and a wonderful human
  being.} liked to say, when I said something about ``errors'' or
``error bars'', that ``they are \emph{uncertainties} not
\emph{errors}! If they were \emph{errors}, we'd correct them!'' That
phrase rings in my head every day. But this points out a difficulty,
which is in the interpretation of uncertainty estimates. When we say
that some parameter is measured to be $42\pm 6$, what does that ``$\pm 6$''
mean?  The answer depends a bit on your statistical philosophy:
If you are a Bayesian, it means that you believe that, with some
fairly well-defined probability (like maybe 68\,percent), the
parameter is within that range.

Seem straightforward? It is, but the straightforwardness of the Bayesian
comes at a cost, which is in assumptions. If you don't want to make some
of those assumptions, you can be a frequentist instead, but then the
interpretation of the measurement $42\pm 6$ is not so simple! For a frequentist
the meaning is that, if the \emph{true}
value of the parameter were $42$,
in some well defined fraction (like maybe 68\,percent)
of hypothetically repeated experiments with similar-quality data the
best-fit or estimated value of the parameter would be within that range.
That is definitely \emph{not} straightforward.
The Bayesian's answer is about the \emph{parameter}, the frequentist's
answer is about hypothetically repeated \emph{counterfactual experiments}.
The trade-off between assumptions made and
simplicity of interpetation will come up again below.
And, combined with this, there are things to say about the terribly named
``confidence interval'' and the even worse-named ``credible region'' and other
abominations.

Most of you, I hope, don't care about such pedantic details and just
want an \emph{uncertainty estimate on your measurement}. In what follows,
I will try to give you that, with enough discussion to answer questions
and explain your choices. We'll start with the theoretical approaches and
then pivot to the empirical approaches.

\section{The standard errors on a least-square fit}

The simplest case for uncertainty estimation---and a case you may have
encountered previously---is a standard, linear fit, as we have discussed endlessly
in this series.\footnote{See, for example, the first two \sectionname s of
  \cite{straightline}. And also the applied math in \cite{gaussianproduct}.}
This is the case in which the following things hold:
\begin{itemize}
\item You have $N$ data points $y_n$. These could be scalars or vectors, but
  let's call them scalars for now.
\item Each data point $y_n$ has an assoicated noise variance
  $\sigma^2_n$, which is the variance of zero-mean Gaussian noise that
  affects each data point $y_n$.  You believe these noise variance
  estimates.\footnote{We will return to the question of testing or
    believing your uncertainties below.}
\item There are no other sources of noise and there is no
  non-Gaussianity (no skewness or kurtosis or outliers or the like) to
  the noise. The noise in data point $y_n$ is independent of the noise
  in any other data point.\footnote{This assumption is the
    easiest---of all these assumptions---to relax.}
\item There is a set of $K$ features $x_{kn}$ for each data point $y_n$ such that
  the expectation for data point $y_n$ is going to be fit as a linear combination
  of these features. That is, the model\footnote{I use the word ``model'' to mean
    a collection of things. It is (at least) a prediction for the data and a
    prediction for the distribution of the noise. A model, for me, is everything
    that you need to construct a likelihood function, or a pdf for the data given
    your parambers.}
  is
  \begin{equation}
    y_n = \sum_{k=1}^K a_k\,x_{kn} + \mbox{noise}
    \quad,
  \end{equation}
  where the noise on data point $y_n$ is drawn, by assumption, from a distribution
  that is Gaussian, zero mean, and variance $\sigma^2_n$.
\end{itemize}
When \emph{all} of these assumptions hold---or, really, when you are
willing to assume \emph{all} of these things---the maximum-likelihood
estimator for the parameters $a_k$ can be found by minimizing what
physicists call ``chi-squared''\footnote{HOGG: Differences between physicists
  and statisticians here.}
\begin{equation}
  \chi^2 = \sum_{n=1}^N \frac{\left[y_n - \sum_k a_k\,x_{kn}\right]^2}{\sigma^2_n}
\end{equation}
The magic is that the maximum-likelihood parameters $a_k$ (HOGG SHOULD I DISTINGUISH
PARAMETERS from PARAMETER ESTIMATES?) are given by straightforward
linear algebra:
\begin{equation}
  \va = [\mX\T\cdot\tC\inv\cdot\mX]\inv\cdot\mX\T\cdot\tC\inv\cdot\vy
\end{equation}
\begin{equation}
  \va\T \equiv \begin{bmatrix} a_1 & a_2 & \hdots & a_K \end{bmatrix}
\end{equation}
\begin{equation}
  \mX \equiv \begin{bmatrix}
    x_{11} & x_{21} & \hdots & x_{K1} \\
    x_{12} & x_{22} & \hdots & x_{K2} \\
    x_{13} & x_{23} & \hdots & x_{K3} \\
    \vdots & \vdots &        & \vdots \\
    x_{1N} & x_{2N} & \hdots & x_{KN}
    \end{bmatrix}
\end{equation}
\begin{equation}
  \tC\inv \equiv \begin{bmatrix}
    1/\sigma^2_1 & 0 & 0 & \hdots & 0 \\
    0 & 1/\sigma^2_2 & 0 & \hdots & 0 \\
    0 & 0 & 1/\sigma^2_3 & \hdots & 0 \\
    \vdots & \vdots & \vdots & & \vdots \\
    0 & 0 & 0 & \hdots & 1/\sigma^2_N \\
    \end{bmatrix}
\end{equation}
\begin{equation}
  \vy\T \equiv \begin{bmatrix} y_1 & y_2 & y_3 & \hdots & y_N \end{bmatrix}
  \quad ,
\end{equation}
where $\va$ is the (column) vector of parameters $a_k$,
$\mX$ is the desgin matrix of features $x_{kn}$,
$\tC\inv$ is the inverse variance tensor (which is diagonal in this simple case),
and
$\vy$ is the (column) vector of data points $y_n$.\footnote{HOGG Note about typesetting!}

...XTX is the inverse covariance matrix.

...Note that XTX is also the second derivative of the LLF.

...Non-linear fit. What's different?

\section{Information theory}

Connection to Cram\'er--Rao bound and Fisher information.

Relationship of first derivative squared to second derivative.

Note inherent frequentism in this point of view.

\section{Bayes}

Foo

You are only permitted to make one kind of uncertainty estimate in Bayes.

So you better believe that model really really well.

Or make it more baroque. (Good option for a problem / exercise!)

\section{Data simulation}

Hello world.

\section{Jackknife and bootstrap}

Hello world.

\section{Nuisance parameters}

Difference between marginalizing and profiling.

Identicality when it comes to the linear, Gaussian case.

What it looks like.

What it looks like in very nonlinear situations (like period fitting).

\section{Systematic error and theoretical uncertainty}

There is only bias and variance; nothing else.

What is a statistical uncertainty? And what is a systematic one?

When do you want to use the former? When do you want to add both in quadrature?

\section{Visualizing and reporting uncertainties}

\section{Common mistakes and troubleshooting}

About 68\,percent of your values should be outside one sigma. What do think
or do if that's not true?

The uncertainty on the mean vs the distribution of values.

My data aren't well fit by my model; what does that mean for my uncertainty
analysis?

Do I multiply my uncertainties up or do I add something in quadrature?

Do I multiply my uncertainties down?

Sometimes you only need the error bars to be correct in a
\emph{relative} sense. If, say they are just being used to weight (by
their squared inverses) data in a fit.

Sometimes you have huge theoretical uncertainties and you want to use
them in your analysis. Sometimes you have these and you \emph{don't} want
to use them in your analysis.

\section{Discussion}

Hello world.

\end{document}
