% This file is part of the Data Analysis Recipes project.
% Copyright 2013 the author.

% to-do
% -----
% - write
% - make examples

\documentclass[12pt,twoside]{article}
\input{../hogg_style}

% header stuff
\renewcommand{\MakeUppercase}[1]{#1}
\pagestyle{myheadings}
\renewcommand{\sectionmark}[1]{\markright{\thesection.~#1}}
\markboth{Inferring distributions from noisy measurements}{}

\newcommand{\data}{D}
\newcommand{\pars}{\theta}
\newcommand{\cv}{\mathrm{CV}}
\newcommand{\pcv}{p_{\cv}}

\begin{document}
\thispagestyle{plain}\raggedbottom
\section*{Data analysis recipes:\ Inferring distributions \\from noisy measurements\footnotemark}

\footnotetext{The \notename s begin on page~\pageref{note:first},
  including the license\note{\label{note:first} Copyright 2013 by the
    author.  You may copy and distribute this document provided that
    you make no changes to it whatsoever.}  and the
  acknowledgements\note{It is a pleasure to thank
      Jo Bovy (IAS),
      Dustin Lang (CMU),
      Dan Foreman-Mackey (NYU),
      Hans-Walter Rix (MPIA),
      Sam Roweis (deceased), and
      Bernhard Sch\"olkopf (MPI-IS)
    for valuable comments and discussions.  This research was
    partially supported by NASA (ADP grant NNX08AJ48G), NSF (grant
    AST-0908357), and a Research Fellowship of the Alexander von
    Humboldt Foundation.  This research made use of the Python
    programming language and the open-source Python packages scipy,
    numpy, and matplotlib.}.}

\noindent
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New~York~University}\\
\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
%% \\[1ex]
%% Jo~Bovy\\
%% \affil{Institute for Advanced Study, Princeton}
%% \\[1ex]
%% Dustin~Lang\\
%% \affil{Princeton University Observatory}\\
%% \affil{Department of Physics, Carnegie Mellon University}

\begin{abstract}
Frequently we have some repeated measurement of a quantity $Q$, one
measurement of $Q$ per object in some sample of objects; we often want
to know the distribution of this quantity in the population from which
the objects are drawn.  The simplest idea is to make a histogram of
the measurements, or estimates, of the quantity.  The measurements are
necessarily noisy, so this empirical histogram of estimates is always
broader or less informative than the true distribution for $Q$.  If
each measurement of $Q$ is based on a likelihood function that
includes an accurate noise model---even in the case that the noise is
different for every measurement---it is possible to replace the simple
histogramming with a hierarchical inference that obtains a
marginalized likelihood (or posterior probability) for the true
distribution of $Q$ that \emph{would have been observed} in a sample
with much less noise per measurement.  The hierarchical inference is a
kind of deconvolution, because it infers a more informative
distribution from less-informative samples---capitalizing on the
distribution information in the set of all samples---but it proceeds
by forward-modeling the data generation process.  We provide some
examples and more strong advice: Investigators sometimes create even
\emph{less} informative (broader) proxies for the distribution than
the empirical histogram of estimates; all such methods are bad.
\end{abstract}

In astronomy, there is a traditional idea that ``bad data are worse
than no data''.  I grew up (academically) hearing this phrase over and
over.  Now, if there is a theme to the \documentnames\ that make up
this series, it \emph{could} be that ``a lot of bad data is as good as
a small amount of good data''.  This theme will justified by the
contents of this \documentname; of course the theme is only factually
correct when the bad data meet certain requirements, especially about
our knowledge---or ability to obtain knowledge---about the noise
contributions that make the bad data bad.

\section{population statistics}

Imagine you have measured the eccentricities of the orbits of a bunch
of exoplanets, or measured the lengths of a large number of fish, or
even tested a group of patients for whether they are carying a
specific disease.  If you are studying the population as a whole, you
want to know as best as possible the \emph{true distribution} of the
quantity you are measuring or testing for.  If your measurements are
perfect (or extremely low in noise) then you don't need to do anything
sophisticated: Make a histogram of the measured eccentricities or
lengths, or compute the fraction that test positive for the disease,
and you are done.  The problem is, in many important situations, your
data just aren't this good.\note{Hogg: Rutherford quotation about
  statistics and getting better data, reference, comment.  Ack BS.}

...keep going with the disease example with some real numbers.

...talk about the ``sophisticated'' idea of co-adding likelihoods.
Note that it makes things \emph{more wrong}.

...do the right thing, and show that it is much better.

...note the extremely strong sensitivity of the conclusions to the
noise model.

\section{estimators built from estimators}

...not terrible idea: histogram of maximum-likelihood estimators.

...when can this give the right distribution?

...why will the empirical variance always be larger than the true variance?

...very bad idea: co-add likelihood functions or histogram posterior
samples.  This is like convolving your noisy data with your errors
once again!  Mistake made surprisingly often.

...concrete example

\section{hierarchical inference}

...the true disribution is your prior

...use the force (probability calculus), Luke

...how in practice to operate

...concrete example, continued from previous section

...relies strongly on good noise information.

\section{using bad data to predict good data}

...use XDQSO data as an example here.

...relies strongly on good noise information for both data sets.

...notes about the fact that accurate information about causality was
necessary to make this work.

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Bovy, Hogg, \& Roweis(2009)]{bovy}
  Bovy,~J., Hogg,~D.~W., \& Roweis, S.~T., 2009,
  Extreme deconvolution: inferring complete distribution functions from noisy, heterogeneous, and incomplete observations, 
  arXiv:0905.2979 [stat.ME]
\end{thebibliography}

\end{document}
