% this file is part of the Data Analysis Recipes project
% Copyright 2012, 2013 David W. Hogg

% to-do
% -----
% - write zeroth draft

% style notes
% -----------
% - ``pdf'' not ``PDF''

\documentclass[12pt,twoside,pdftex]{article}
\input{./hogg_style}

% header stuff
\renewcommand{\MakeUppercase}[1]{#1}
\pagestyle{myheadings}
\renewcommand{\sectionmark}[1]{\markright{\thesection.~#1}}
\markboth{Extracting information from images}{Introduction}

\begin{document}
\thispagestyle{plain}\raggedbottom
\section*{Data analysis recipes:\\
  Making decisions}

\footnotetext{%
  The \notename s begin on page~\pageref{note:first}, including the
  license\note{\label{note:first}%
    Copyright 2012, 2013 David W. Hogg (NYU).  Right now this document
    (though publicly available) is NOT FOR DISTRIBUTION.  Eventually,
    and once it is ready, I will release it with the license ``You may
    copy and distribute this document provided that you make no
    changes to it whatsoever''.}
  and the acknowledgements\note{%
    It is a pleasure to thank
      Jo Bovy (IAS),
      Dan Foreman-Mackey (NYU),
      Dustin Lang (CMU),
      Phil Marhall (Oxford), and
      Sam Roweis (deceased)
    for discussions and comments.  This
    research was partially supported by the US National Aeronautics
    and Space Administration and National Science Foundation.}}

\noindent
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}\\
and~\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
%% \\[1ex]
%% A. Nother Author
%% \affil{Oxford University}

\begin{abstract}
Probabilistic inference at its best gives probabilistic information
about parameters and models.  In many cases of importance---as with
making a discovery or experimental design or purchasing or delivery of
results---it is necessary to make a hard decision or choice.  It is
almost never the right move to choose the \emph{most probable} model
or option; it is rare that the most probable is the option that
delivers the most benefit to the decider.  Instead the decider ought
to specify a utility function and work to optimize expected utility,
or minimize expected loss, or minimize maximal loss.  In real-world
situations of building experiments or operating an organization, to be
generally useful the utility must be specified in (the equivalent of)
currency (dollar) units.  Whether the decision is about which
observation to make next, which of several hypotheses to reject, or
which of very different research programs to pursue, the employed
utility function should be an approximation to the decider's
\emph{long-term future discounted free cash flow}.
\end{abstract}

\section{You never decide on probability alone}

Imagine you have taken imaging of a patch of the sky and you want to
know the brightness of a particular star, or you have performed a
survey of consumers and you want to know what fraction of them prefer
a particular product.  In each case you have noisy data, which
contains noisy information about the quantity you care about.  The
scientifically responsible thing to report is the full likelihood
function or the posterior probability distribution
function\endnote{Because the computation of a posterior pdf involves
  additional assumptions over and above the assumptions made in
  computing a likelihood, if you release only posterior pdfs, you must
  always \emph{also} report the prior pdf you employed.  I discuss
  this more in other contributions in this series.} (pdf) for the the
quantity of interest---the star brightness or the consumer fraction.
Only these full functions properly transmit the full information
content of the measurement.

Nonetheless, in various situations, it is necessary to deliver \emph{a
  particular value} for the quantity of interest.  For example, you
might be calculating a telescope exposure time (for your next
measurement) or deciding how many units of a product to ship to a
store.  In each case, you have to use \emph{some value} to perform the
calculation.  Well, strictly speaking, you don't \emph{have} to use a
single value to do this calculation, but in practice it is often the
only practicable way.  For example, sometimes you have a superior
officer who demands a number, or a referee who insists that you put a
number in the abstract of your paper.  When you are in this situation,
what number should you report or use?  The rules of probabilistic
inference \emph{do not answer this question}.  The rules of
probabilistic inference only tell you how to manipulate probabilities
and frequencies for data and parameters.  For example, they don't tell
you whether to report the mode, mean, or median of the posterior pdf
or the maximum of the likelihood or some percentile upper or lower
limit.  (And they don't tell you how to adjust if the likelihood or
posterior is peaked somewhere that seems unlikely or embarassing or
likely to get you fired, demoted, ridiculed, or bankrupt.)

Similarly, in discovery or detection: Do you take ``three sigma'' or
``five sigma'' or ``ten'' to be the threshold?  Probabilistic
inference does not answer this question either.  If it is bad for you
to report the existence of something that isn't there, you will set
your threshold high.  If it is important that you \emph{not miss}
something that \emph{is} there, you will set it low.  As these
examples illustrate, it is your \emph{utility}---what is useful to you
or important to you---that informs your decision, not just the results
of the probabilistic inference.

Most importantly, if you are trying to decide \emph{between models} or
\emph{between hypotheses}, and these models have different posterior
probabilities (possibly marginalized over all parameters) how do you decide?  Once again,
the rules of probabilistic inference do not tell you which model to
choose.  You can choose the more probable model, but that is rarely
\emph{required}.  It is often not even sensible: What if model $A$ is
slightly more probable, but requires thousands of CPU-years to
compute, whereas model $B$ is slightly less probable but you can
compute it on your mobile phone?  For many purposes, model $B$ would
be the model to choose.  Economic factors (and we will get very
specific about what those factors are) must play a role.

Finally, and as we have argued elsewhere\note{where?}, if you can
\emph{possibly avoid} making a decision you should.  The laws of
probabilistic inference tell you how to \emph{mix} the models or
marginalize over unknowns; if you can live with a mixture you should.
Only decide when you have no choice, as when you have to put a number
in an abstract, type a line in a published table or chart, or cut
metal in an experiment or instrument.  It is only then---when
irreversible action is imminent---that choice is \emph{really}
required.

\section{Money must be involved}

Back in 2005-ish, the cosmology community underwent a soul-searching
exercise about future projects intended to investigate the nature of
\emph{dark energy}, or the mechanical cause of the late-time
acceleration of the Universe's expansion.  The Dark-Energy Task
Force (DETF) ended up recommending that projects be ranked on the
basis of their ability to constrain two dark-energy parameters, $w_0$
and $w_a$.  In particular, they argued that the relevant scalar or
utility or figure of merit (FoM) to consider is the inverse of the
area inside the 95-percent confidence interval (implicitly a
likelihood contour) in the $w_0$--$w_a$ plane.\note{\cite{detf}.}

In many ways this was a breakthrough: A committee recommends a
quantitative method for comparing (necessarily expensive, complex, and
lengthy) projects, and describes that quantitative method in terms of
merit or utility.  For this the DETF deserves to be commended.
Furthermore, the proposed FoM scalar relates to the informaton content
in the data delivered by the experiments, so it is eminently sensible.

However, there is an equally valid point of view in which this
recommendation was \emph{useless}.  The reason is that although the
FoM permits the community to compare projects in terms of information
delivered about dark energy, no two projects deliver the same FoM, no
two projects have the same total cost, and no two projects deliver
results at the same point in the future.  If one project delivers an
FoM of 1200, costs $8\times 10^6$~USD, and takes four years, and
another delivers an FoM of 1950 and costs $17\times 10^6$~USD, and
takes 3.5 years, how are we to compare them?  We need to be able to
understand the trade-offs between money, FoM, and time.  And sure
enough, although the DETF was able to classify projects into
categories (FoM bins), it was not able to rank the relative value of
very qualitatively different projects.

...similarly spectroscopic fibers in \project{SDSS}...

...Automated decision making for follow-up...

...Objective experimental design...

\section{What is a decision?}

\section{Utility}

minimax and utility maximization

\section{Long-term future discounted free cash flow}

Long-term future.

Discounted.

Free cash flow.

Assumptions under which this is possible.

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}\raggedright
\bibitem[Dark Energy Task Force(2006)]{detf}
  Dark Energy Task Force, 2006,
  ``Report of the Dark Energy Task Force'', arXiv:astro-ph/0609591
\bibitem[Hogg \etal(2010a)]{straightline}
  Hogg,~D.~W., Bovy,~J., \& Lang,~D., 2010a,
  ``Data analysis recipes:\ Fitting a model to data'', arXiv:1008.4686
\end{thebibliography}

\end{document}
