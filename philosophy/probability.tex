% This file is part of the Data Analysis Recipes project.
% Copyright 2012 David W. Hogg (NYU)

% to-do items
% -----------
% - explain the over-conditioning problem with p(a|b,c) p(b|a,c).

% style notes
% -----------
% - when at the end of the sentence, put the \endnote AFTER the period
% - when at the end of a phrase, put the \endnote BEFORE the comma or parens
% - make sure the endnotes can be read on their own, outside of context
% - careful with the words ``error'', ``uncertainty''
% - careful with the words ``probability'', ``frequency'', ``likelihood''
% - use () for function arguments, and [] for grouping/precedence
% - define macros; remember 1, 2, infinity
%   - (check out my awesome \given macro)
% - put new terms in \emph{}, put only referred-to words in quotation marks.
% - do in-text itemized lists with \textsl{(a)}~ and so on.

\documentclass[12pt,twoside]{article}
\usepackage{amssymb,amsmath,mathrsfs,../hogg_endnotes,natbib}
\usepackage{float,graphicx}

\setlength{\emergencystretch}{2em}%No overflow

\newcommand{\notenglish}[1]{\textsl{#1}}
\newcommand{\aposteriori}{\notenglish{a~posteriori}}
\newcommand{\apriori}{\notenglish{a~priori}}
\newcommand{\adhoc}{\notenglish{ad~hoc}}
\newcommand{\etal}{\notenglish{et al.}}

\newcommand{\documentname}{document}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\figurenames}{\figurename s}
\newcommand{\problemname}{Exercise}
\newcommand{\problemnames}{\problemname s}
\newcommand{\solutionname}{Solution}
\newcommand{\notename}{note}

\newcommand{\note}[1]{\endnote{#1}}
\def\enotesize{\normalsize}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % the ONE footnote needs this

\newcounter{problem}
\newenvironment{problem}{\paragraph{\problemname~\theproblem:}\refstepcounter{problem}}{}
\newcommand{\affil}[1]{{\footnotesize\textsl{#1}}}

% matrix stuff
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
% parameter vectors
\newcommand{\parametervector}[1]{\mmatrix{#1}}
\newcommand{\pvtheta}{\parametervector{\theta}}
% set stuff
\newcommand{\setofall}[3]{\{{#1}\}_{{#2}}^{{#3}}}
\newcommand{\allq}{\setofall{q_i}{i=1}{N}}
% other random multiply used math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}

% header stuff
\renewcommand{\MakeUppercase}[1]{#1}
\pagestyle{myheadings}
\renewcommand{\sectionmark}[1]{\markright{\thesection.~#1}}
\markboth{Probability calculus}{Introduction}

\begin{document}
\thispagestyle{plain}\raggedbottom
\section*{DRAFT 2012-03-16:\ Data analysis recipes:\ \\
  Probability calculus\footnotemark}

\footnotetext{%
  The \notename s begin on page~\pageref{note:first}, including the
  license\note{\label{note:first}%
    Copyright 2012 David W. Hogg (NYU).  You may copy and distribute this
    document provided that you make no changes to it whatsoever.}
  and the acknowledgements\note{%
    It is a pleasure to thank
      [insert names here]
    for discussions and comments that shaped these ideas.  This
    research was partially supported by the US National Aeronautics
    and Space Administration and National Science Foundation.}}

\noindent
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}\\
\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
%% \\[1ex]
%% A. Nother Author
%% \affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}

\begin{abstract}
I review the rules by which probability distribution functions can
(and cannot) be combined. I connect these rules to the operations
performed in probabilistic data analysis.  Dimensional analysis is
emphasized as a valuable tool for helping to construct non-wrong
probabilistic statements.
\end{abstract}

\noindent
When constructing the plan or basis for a probabilistic inference---a
data analysis making use of likelihoods and also prior PDFs and
posterior PDFs and also marginalization of nuisance parameters---the
options are \emph{incredibly strongly constrained} by the simple rules
of probability calculus.  That is, there are only certain ways that
probability distribution functions (``PDFs'' in what follows)
\emph{can} be combined to make new PDFs, compute expectation values,
or make other kinds of non-wrong statements.  For this reason, it
behooves the data analyst to have good familiarity and facility with
these rules.

Formally, probability calculus is extremely simple.  However, it is not
always part of a physicist's (or biologist's or chemist's or
economist's) education; so I am going to give a very fast review right
here.  The key idea I want to convey is that if you think about the
units or dimensions of the things you are using, you almost never go
wrong.

For space and specificity---and because it is most useful for most
problems I encounter---I will focus on continuous variables (think
``model parameters'' for the purposes of inference) rather than
binary, integer, or discrete parameters, although I might say one or
two things here or there.  I also won't always be specific about the
domain of variables (for example whether a variable $a$ is defined on
$0<a<1$ or $0<a<\infty$ or $-\infty<a<\infty$); the limits of
integrals---all of which are definite---will usually be
implicit.\note{In my world, \emph{all integrals are definite
    integrals}.  The integral is never just the anti-derivative, it is
  always a definite, finite, area or volume or measure.  That has to
  do with the fact, somehow, that the rules of probability calculus
  are an application of the rules of measure theory.}

\section{Generalities}

Probability distribution functions have units or dimensions.  Don't
ignore them.  For example, if you have a continuous parameter $a$, and
a PDF $p(a)$ for $a$, it must obey the normalization condition
\begin{eqnarray}\displaystyle
1 &=& \int p(a)\,\dd a
\quad ,
\end{eqnarray}
where the limits of the integral should be thought of as going over
the entire domain of $a$.  This is almost the \emph{definition} of a
PDF, from my (pragmatic, informal) point of view.  This normalization
condition shows that $p(a)$ has units of $a^{-1}$.  Nothing else would
integrate properly to a dimensionless result.  Even if $a$ is a
multi-dimensional vector or list or tensor or field or even point in
function space, the PDF must have units of $a^{-1}$.

In the multi-dimensional case, the units of $a^{-1}$ are found by
taking the product of all the units of all the dimensions.  So, for
example, if $a$ is a six-dimensional phase-space position in
three-dimensional space (three cartesian position components measured
in m and three cartesian momentum components measured in
kg\,m\,s$^{-1}$), the units of $p(a)$ would be
kg$^{-3}$\,m$^{-6}$\,s$^3$.

Most problems we will encounter will have multiple parameters; even if
we \emph{condition} $p(a)$ on some particular value of another
parameter $b$, that is, ask for the PDF for $a$ \emph{given} that $b$
has a particular, known value to make $p(a \given b)$ (read ``the PDF
for $a$ given $b$''), it must obey the same marginalization
\begin{eqnarray}\displaystyle
1 &=& \int p(a \given b)\,\dd a
\quad ,
\end{eqnarray}
but you can \emph{absolutely never do} the integral
\begin{eqnarray}\displaystyle
\mbox{\textbf{wrong:}} & & \int p(a \given b)\,\dd b
\end{eqnarray}
because that integral would have units of $a^{-1}\,b$, which is (for
our purposes) absurd.  Despite its absurdity, this integral has been
performed many times in error in the literature of data analyses.

If you have a probability distribution for two things (``the PDF for
$a$ and $b$''), you can always factorize it into two distributions,
one for $a$, and one for $b$ given $a$ or the other way around:
\begin{eqnarray}\displaystyle
p(a, b) &=& p(a)\,p(b \given a)
\\
p(a, b) &=& p(a \given b)\,p(b)
\quad ,
\end{eqnarray}
these two factorizations, taken together lead to what is sometimes
called ``Bayes's theorem'' but is just a simple consequence of the
factorization:
\begin{eqnarray}\displaystyle
p(a \given b) &=& \frac{p(b \given a)\,p(a)}{p(b)}
\quad ,
\end{eqnarray}
where the ``divide by $p(b)$'' aspect of that gives many philosophers
and mathematicians the chills (though certainly not me).\note{Division
  by zero is a huge danger---in principle---when applying Bayes's
  theorem.  In practice, if there is support for the model in your
  data, or support for the data in your model, you don't hit any
  zeros.}  Conditional probabilities factor just the same as
unconditional ones (and many will tell you that there is no such thing
as an unconditional probability\note{There are no unconditional
  probabilities!  This is because whenever \emph{in practice} you
  calculate a probability or a PDF, you are always making strong
  assumptions.  Your probabilities are all conditioned on these
  assumptions.}); they factor like this:
\begin{eqnarray}\displaystyle
p(a, b \given c) &=& p(a \given c)\,p(b \given a, c)
\\
p(a, b \given c) &=& p(a \given b, c)\,p(b \given c)
\\
p(a \given b, c) &=& \frac{p(b \given a, c)\,p(a \given c)}{p(b \given c)}
\quad ,
\end{eqnarray}
where the condition $c$ must be carried through all the terms; the
whole right-hand side must be conditioned on $c$ if the left-hand side
is.  Again, there was Bayes's theorem, and you can see its role in
conversions of one kind of conditional probability into another.  For
technical reasons, I usually write Bayes's theorem like this:
\begin{eqnarray}\displaystyle
p(a \given b, c) &=& \frac{1}{Z}\,p(b \given a, c)\,p(a \given c)
\\
Z &\equiv& \int p(b \given a, c)\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}

Here are things you \emph{can't} do:
\begin{eqnarray}\displaystyle
\mbox{\textbf{wrong:}} & & p(a \given b, c)\,p(b \given a, c)
\\
\mbox{\textbf{wrong:}} & & p(a \given b, c)\,p(a \given c)
\quad;
\end{eqnarray}
the first over-conditions (it is not a factorization of anything
possible) and the second has units of $a^{-2}$, which is absurd (for
our purposes).  Know these and \emph{don't do them}.

One important and confusing point about all this: The terminology used
throughout this \documentname\ \emph{enormously overloads} the symbol
$p(\cdot)$.  That is, we are using, in each line of this discussion,
the function $p(\cdot)$ to mean something different; it's meaning is
set by the letters used in its arguments.  That is a nomenclatural
abomination.\note{Serious, non-ambiguous mathematicians often
  distinguish between the name of the variable and the name of a draw
  from the probability distribution for the variable, and then they
  can write things like $p_{A|BC}(a \given b, c)$ to distinguish the
  functions.  This permits, for example, another thing $q$ to be drawn
  from the same distribution as $a$ by a notation like $p_{A|BC}(q
  \given b, c)$.  In our (very bad) notation $p(q \given b, c)$ would
  in general be different from $p(a \given b, c)$ because we take the
  meaning of the function from the names of the argument variables.
  See why that is very bad?.  Some explicit subscripting policy seems
  like much better practice and we should probably all adopt something
  like it, though I won't here.\par Another good idea is to make the
  probability equations be about statements, so instead of writing
  $p(a \given b, c)$ you write $p(A=a \given B=b, C=c)$.  This is
  unambiguous---you can write useful terms like $p(A=q \given B=b,
  C=c)$ to deal with the $a$, $q$ problem---but it makes the equations
  big and long.}  I apologize, but it is so standard in our business I
won't change.

``Measure theory'' (for me, anyway\note{Have you noticed that I am
  \emph{not} a mathematician?}) is the theory of things in which you
can do integrals.  You can ``integrate out'' variables you want to get
rid of (or, in what follows, \emph{not} infer) by integrals that look
like
\begin{eqnarray}\displaystyle\label{eq:marginalize}
p(a \given c) &=& \int p(a \given b, c)\,p(b \given c)\,\dd b
\quad ,
\end{eqnarray}
where again the integrals go over the entire domain of $b$ in each
case, and again if the left-hand side is conditioned on $c$, then
everything on the right-hand side must be also.  This equation is a
natural consequence of the things written above and dimensional
analysis.  Recall that because $b$ is some kind of arbitrary, possibly
very high-dimensional mathematical object, these integrals can be
extremely daunting in practice (see below).  Sometimes equations like
(\ref{eq:marginalize}) can be written
\begin{eqnarray}\displaystyle
p(a \given c) &=& \int p(a \given b)\,p(b \given c)\,\dd b
\quad ,
\end{eqnarray}
where the dependence of $p(a \given b)$ on $c$ has been dropped.  This
is only permitted if it \emph{happens to be the case} that $p(a \given
b, c)$ doesn't, in practice, depend on $c$.  The dependence on $c$ is
really there (in some sense), it just might be trivial or null.

In rare cases, you can get factorizations that look like this:
\begin{eqnarray}\displaystyle
p(a, b \given c) &=& p(a \given c)\,p(b \given c)
\quad ;
\end{eqnarray}
this factorization doesn't have the PDF for $a$ depend on $b$ or vice
versa.  When this happens---and it is rare---it says that $a$ and $b$
are ``independent'' (at least conditional on $c$).\note{The word
  ``independent'' has many meanings in different contexts of
  mathematics and probability; I will avoid it in what follows, except
  in the ``iid'' or ``independent and identically distributed''
  context.  I prefer the word ``separable'' for this situation.}  In
many cases of data analysis, in models of data sets, we often have a
large number of data $a_n$, indexed by $n$, each of which is not only
independent in this sense, but also drawn from the same distribution
function.  Data of this form are called ``independent and identically
distributed'' or ``iid'' in the literature.  If you have a set of $N$
iid data $a_n$, each drawn from a probability distribution $p(a_n
\given c)$, then the probability of the full data set is simply the
product of the individual data-point probabilities:
\begin{eqnarray}\displaystyle
p(\setofall{a_n}{n=1}{N} \given c) &=& \prod_{n=1}^N p(a_n \given c)
\quad ;
\end{eqnarray}
this is really the definition, in some sense, of ``iid''.

I am writing here mainly about continuous variables, but one thing
that comes up frequently in data analysis are ``mixture models'' in
which data are produced by two (or more) qualitatively different
processes (some data are good, and some are bad, for example) that
have different relative probabilities.  When a variable ($b$, say) is
discrete, the marginalization integral corresponding to
\equationname~(\ref{eq:marginalize}) becomes a sum
\begin{eqnarray}\displaystyle
p(a \given c) &=& \sum_b p(a \given b, c)\,p(b \given c)
\quad ,
\end{eqnarray}
and the normalization of $p(b \given c)$ becomes
\begin{eqnarray}\displaystyle
1 &=& \sum_b p(b \given c)
\quad ;
\end{eqnarray}
in both sums, the sum is implicitly over the (small number of)
possible states of $b$.

If you have a conditional PDF for $a$ like $p(a \given c)$ and you
want to know the expectation value $E(a \given c)$ of $a$ under this
PDF (which would be, for example, something like the mean value of $a$
you would get if you drew many draws from the conditional PDF), you
just integrate
\begin{eqnarray}\displaystyle
E(a \given c) = \int a\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}
This generalizes to any function $f(a)$ of $a$:
\begin{eqnarray}\displaystyle
E(f \given c) = \int f(a)\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}
You can see the marginalization integral (\ref{eq:marginalize}) that
converts $p(a \given b, c)$ into $p(a \given c)$ as providing the
\emph{expectation value} of $p(a \given b, c)$ under the conditional
PDF $p(b \given c)$.  That's deep and relevant for what follows.

\begin{problem}
You have conditional PDFs $p(a \given d)$, $p(b \given a, d)$, and
$p(c \given a, b, d)$.  Write expressions for $p(a, b \given d)$, $p(b
\given d)$, and $p(a \given c, d)$.
\end{problem}

\begin{problem}
You have conditional PDFs $p(a \given b, c)$ and $p(a \given c)$
expressed or computable for any values of $b$ and $c$.  You are not
permitted to multiply these together, of course.  But can you use them
to infer the conditional PDF $p(b \given a, c)$ or $p(b \given c)$?
\end{problem}

\begin{problem}
You have a function $g(b)$ that is a function only of $b$.  You have
conditional PDFs $p(a \given c)$ and $p(b \given a, c)$.  What is the
expectation value $E(g \given c)$ for $g$ conditional on $c$ but
\emph{not} conditional on $a$?
\end{problem}

\section{Inference}

Repeat everything with a likelihood, a prior, and something
hierarchical \ldots

\section{Example}

Give a simple data analysis problem and show that you have no real
choice---except in what models to consider! \ldots

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Jaynes(2003)]{jaynes}
  Jaynes,~E.~T., 2003,
  \textit{Probability theory:\ The logic of science} (Cambridge University Press)
\bibitem[Mackay(2003)]{mackay}
  Mackay,~D.~J.~C., 2003,
  \textit{Information theory, inference, and learning algorithms} (Cambridge University Press)
\bibitem[Press \etal(2007)]{press}
  Press,~W.~H., Teukolsky,~S.~A., Vetterling,~W.~T., \& Flannery,~B.~P., 2007,
  \textit{Numerical recipes:\ The art of scientific computing} (Cambridge University Press)
\bibitem[Sivia \& Skilling(2006)]{sivia}
  Sivia,~D.~S. \& Skilling,~J., 2006,
  \textit{Data analysis:\ A Bayesian tutorial} (Oxford University Press)
\end{thebibliography}

\end{document}
